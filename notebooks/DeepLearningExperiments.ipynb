{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Deep Learning Experiments using IBM Watson DLaaS\nIn this notebook, we illustrate how to run deep learning experiments using Deep Learning as a Service (DLaaS) capabilities in Watson Machine Learning.\n\nIn order to leverage IBM DLaaS, you will need to have two IBM Cloud services in addition to Watson Studio where you're running this notebook:\n- **Cloud Object Storage (COS)** which serves as storage for the training data as well as training results and logging/monitoring data. Actually, COS is needed for all Watson Studio prohects.\n- **Watson Machine Learning (WML)** which handles sotring the training and experiment information, executing the training runs and experiments, and deploying trained models.\n\nTraining a deep learning neural networks involves the following steps:\n1. Setup COS to define buckets for reading the training data and buckets for writing the training results.\n2. Create one or more training definition which outline the neural network (NN) architecture and the references to the COS bucket containing the input training data and output COS bucket for writing training results.\n3. **Optional** Execute a training run based on the training definition in step 2. This step is optional and only executed to validate the information is set up correctly for training. Practically, data scientists would run experiments with hyper parameter optimization (HPO) which consist of multiple training definitions and multiple parameters to optimize. In this notebook, we will skip executing a separate training run and focus on running an experiment.\n4. Create an experiment definition which would include the training definition in step 2, the COS bucket with input training data, the COS bucket to write the training results, and the optimization configuration parameters such as the optimization algorithm, parameters to vary, and the metrics to consider when comparing alternatives.\n5. Monitor / visualize experiment results which is a critical step to understand the performance of your models.\n\nIn the rest of this notebook, we will go through these steps and discuss the details for each step.\n\n1. [Credentials Setup](#creds_setup)\n\n    1.1. [IBM Cloud Object Storage](#cos)   \n    1.2. [Watson Machine Learning Setup](#wml)\n    \n    \n2. [Training Definition](#training_def)\n\n\n3. [Setup & Run Experiments](#exp)\n\n\n4. [Store & Deploy Trained ML Models](#store)\n\n\n5. [Summary](#summary)\n", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "## DLaaS PreRequisites\nPlease review the [DLaaS Prerequisites](https://github.com/joe4k/deeplearningutils/blob/master/DLaaS_Prerequisites.pdf) tutorial which explains how to setup your IBM Cloud account and create the required IBM Cloud services so you're able to run deep learning training experiments using IBM Watson DLaaS capabilities.\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"creds_setup\"></a>\n## 1. Credentials Setup\n\n<a id=\"cos\"></a>\n### 1.1 IBM Cloud Object Storage\nIn this section, we explain how to work with Cloud Object Storage (COS) for purposes of running deep learning training experiments.\n\nPlease make sure you went through the [DLaaS Prerequisites](https://github.com/joe4k/deeplearningutils/blob/master/DLaaS_Prerequisites.pdf) tutorial before proceeding. Assuming you've run through those steps, then you should have a Cloud Object Storage (COS) instance with the associated credentials.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 1.1.1 COS Credentials\nIn the next cell, you need to specify the cloud object storage instance credentials. The DLaaS Prerequisities tutorial explains how to get those credentials.\n\nThe following link also offers instructions for creating the credentials for your Cloud Object Storage instance:\nhttps://github.com/biosopher/unofficial-watson-studio-python-utils/wiki/Save-COS-Credentials-to-cos_credentials.json\n\n**Note** Make sure you use the {\"HMAC\":true} parameter when creating the credentials.\n\nThe COS credentials should look as follows:\n```\n{\n  \"apikey\": \"********************\",\n  \"cos_hmac_keys\": {\n    \"access_key_id\": \"*************************\",\n    \"secret_access_key\": \"*************************\"\n  },\n  \"endpoints\": \"https://cos-service.bluemix.net/endpoints\",\n  \"iam_apikey_description\": \"***********************\",\n  \"iam_apikey_name\": \"*****************************\",\n  \"iam_role_crn\": \"***************************\",\n  \"iam_serviceid_crn\": \"****************************\",\n  \"resource_instance_id\": \"********************************\"\n} ```\n\nAdditionally, you need to specify the service endpoint for your COS instance. To get that endpoint:\n\n- Navigate to your COS instance on your IBM Cloud account\n- Click on the Endpoint link in the left navigation column\n- Copy the public endpoint corresponding to your COS location. If your location is us-geo, then select the public endpoint for us-geo.\n\nThe service endpoint would look as follows: 'https://s3-api.us-geo.objectstorage.softlayer.net'", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# replace with your credentials\ncos_credentials = {\n  \"apikey\": \"**************\",\n  \"cos_hmac_keys\": {\n    \"access_key_id\": \"**************\",\n    \"secret_access_key\": \"**************\"\n  },\n  \"endpoints\": \"https://cos-service.bluemix.net/endpoints\",\n  \"iam_apikey_description\": \"**************\",\n  \"iam_apikey_name\": \"**************\",\n  \"iam_role_crn\": \"**************\",\n  \"iam_serviceid_crn\": \"**************\",\n  \"resource_instance_id\": \"**************\"\n}\nservice_endpoint = 'https://s3-api.us-geo.objectstorage.softlayer.net'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "<a id=\"wml\"></a>\n### 1.2 Watson Machine Learning (WML) Setup\nNext setup the access to WML which we'll use to setup the Deep Learning experiments.\n\nThe Watson Machine Learning credentials can be obtained from IBM Cloud account by finding the specific WML service instance and clicking the Service credentials in the left navigation column.\n\nFor more details on creating a Watson Machine Learning service and getting the credentials, please consult the [DLaaS Prerequisites](https://github.com/joe4k/deeplearningutils/blob/master/DLaaS_Prerequisites.pdf) tutorial.\n\nWML credentials look as follows:\n\n```\nwml_credentials = {\n  \"apikey\": \"************\",\n  \"iam_apikey_description\": \"************\",\n  \"iam_apikey_name\": \"************\",\n  \"iam_role_crn\": \"************\",\n  \"iam_serviceid_crn\": \"************\",\n  \"instance_id\": \"************\",\n  \"password\": \"************\",\n  \"url\": \"************\",\n  \"username\": \"************\"\n}```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Specify the WML credentials\n# Copy paste the credentials json from your service instance\nwml_credentials = {\n  \"apikey\": \"********\",\n  \"iam_apikey_description\": \"********\",\n  \"iam_apikey_name\": \"********\",\n  \"iam_role_crn\": \"********\",\n  \"iam_serviceid_crn\": \"********\",\n  \"instance_id\": \"********\",\n  \"password\": \"********\",\n  \"url\": \"https://us-south.ml.cloud.ibm.com\",\n  \"username\": \"********\"\n}"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Load Python package to simplify working with COS\nimport boto3"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Define a client for your COS instance based on the credentials\ncos_client = boto3.client('s3', \n                          endpoint_url = service_endpoint, \n                          aws_access_key_id=cos_credentials[\"cos_hmac_keys\"][\"access_key_id\"], \n                          aws_secret_access_key=cos_credentials[\"cos_hmac_keys\"][\"secret_access_key\"])"
        }, 
        {
            "source": "### 1.2 COS Utilities\nIn the next cell, we define multiple utilities that are useful when working with Cloud Object Storage.\n\n- **get_all_buckets** returns all the buckets created in your COS instance.\n- **get_objects_in_bucket** returns all the objects in a specific bucket in your COS instance.\n- **create_unique_bucket** creates a new bucket in your COS instance.\n- **upload_file_to_bucket** uploads file from the local notebook environment to a bucket in your COS instance.\n- **download_file_from_bucket** downloads file from the bucket in your COS instance.\n- **download_file_from_url** downloads file from a given url to the local notebook environment.\n- **remove_files_from_dir** removes files from a local directory; mainly used to clean up files when no longer needed.\n\nIf the training data is provided via a URL, then you can use the download_file_from_url and upload_file_to_bucket to get the data to your COS bucket.\n\nIf the training data is provided via a COS bucket, then you can use the download_file_from_bucket and upload_file_to_bucket to get the data to your COS bucket. It may be better to just use the data in the COS bucket specified as opposed to copying to your own COS bucket.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# load some require python packages\nimport random\nimport string\nimport os\nimport urllib\n\n# Return all buckets in your COS instance\ndef get_all_buckets(cos_client):\n    response = cos_client.list_buckets()\n    allbuckets = []\n    for bucket in response['Buckets']:\n        allbuckets.append(bucket['Name'])\n    return allbuckets\n\n# Return all the objects in a COS bucket\ndef get_objects_in_bucket(cos_client,bucket_name):\n    return cos_client.list_objects(Bucket=bucket_name)\n\n# Create a unique COS bucket\ndef create_unique_bucket(cos_client, bucket_prefix):\n    # Create a random 10 digit string\n    # this random string increases the likelihood of the bucket name to be unique\n    lst = [random.choice(string.ascii_letters + string.digits) for n in range(10)]\n    random_string = \"\".join(lst).lower()\n    bucket = \"%s-%s\" % (bucket_prefix, random_string)\n    \n    #print(\"creating bucket: \", bucket)\n    cos_client.create_bucket(Bucket=bucket)\n    print(\"Bucket %s created\" % bucket)\n    return bucket\n\n# Upload objects to COS bucket\ndef upload_file_to_bucket(cos_client,file,bucket):\n    file_name = os.path.basename(file)\n    print(\"Uploading %s to bucket: %s\" % (file_name,bucket))\n    cos_client.upload_file(file, bucket, file_name)\n\n# Download objects from COS bucket\ndef download_file_from_bucket(cos_client, bucket, file_to_download, save_path, is_redownload=False):\n    if not os.path.exists(save_path) or is_redownload:\n        with open(save_path, 'wb') as file:\n            print(\"Downloading %s\" % file_to_download)  # \"\\r\" allows us to overwrite the same line\n            try:\n                cos_client.download_fileobj(bucket, file_to_download, file)\n            except:\n                e = sys.exc_info()[0]\n                print(e.__dict__)\n                if e.response != None:\n                    print(\"Detailed error: \", e.response)\n                print('An error occured downloading %s from %s' % (file_to_download, bucket))\n                os.remove(local_file)\n            finally:\n                file.close()\n\n# Download objects from a URL \ndef download_file_from_url(file_url,save_directory=None):\n    # If save directory provided then don't delete local downloads\n    working_directory = \"temp_cos_files\"\n    if save_directory is not None:\n        working_directory = save_directory\n    os.makedirs(working_directory, exist_ok=True)\n\n    file_name = os.path.basename(file_url)\n    # Sometime url include parms and need to split those off to get valid file_name\n    file_name = file_name.split('?')[0]\n    # Delete file if present as perhaps download failed and file corrupted\n    file_path = os.path.join(working_directory, file_name)\n    if os.path.exists(file_path):\n        os.remove(file_path)\n\n    file_path, _ = urllib.request.urlretrieve(file_url, file_path)\n    stat_info = os.stat(file_path)\n    print('Downloaded', file_path, stat_info.st_size, 'bytes.')\n    \n    \n# Remove all files from the specified directory in the local environment\ndef remove_all_files_from_dir(dir):\n    for f in os.listdir(dir):\n        file_path = os.path.join(dir, f)\n        if os.path.exists(file_path):\n            os.remove(file_path)\n            \n# Remove a specific file from a given dir\ndef remove_file_from_dir(filename, dir):\n    file_path = os.path.join(dir, filename)\n    if os.path.exists(file_path):\n        os.remove(file_path)\n            "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# List all buckets in your COS instance\nbuckets = get_all_buckets(cos_client)\nprint(buckets)"
        }, 
        {
            "source": "### 1.3 Training Data\nCreate COS buckets, one for the training data and the other for the training results, and upload the MNIST training and test data to the COS bucket for training data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create two COS buckets\n# One bucket to store the input training data\nmnist_training_data_bucket_prefix = 'mnist-training-data'\nmnist_training_data_bucket = create_unique_bucket(cos_client,mnist_training_data_bucket_prefix)\n# One bucket to write the output training results\nmnist_training_results_bucket_prefix = 'mnist-training-results'\nmnist_training_results_bucket = create_unique_bucket(cos_client,mnist_training_results_bucket_prefix)\n\n# Note that the create bucket method appends a random 10 character string so the bucket name is more likely to be unique\n\n\n# Download training data from the following URLs and upload to COS bucket\ndata_links = ['http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n              'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n              'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n              'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz']\n\nbucketName = mnist_training_data_bucket\nworking_dir = \"mnist_files\"\nallfiles = []\nfor file_url in data_links:\n    file_name = os.path.basename(file_url)\n    #allfiles = allfiles.append(file_name)\n    allfiles.append(file_name)\n    print(\"file url: %s \" % file_url)\n    print(\"file name: %s \" % file_name)\n    download_file_from_url(file_url,working_dir)\n    file_path = os.path.join(working_dir, file_name)\n    upload_file_to_bucket(cos_client,file_path,bucketName)\n\nfor f in allfiles:\n    remove_file_from_dir(f, working_dir)"
        }, 
        {
            "source": "At this point, you should have two COS buckets, one for the input training data and one for the output training results.\n\nThe input training data bucket should also have the training data you'd like to use for your deep learning experiments.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# List all objects in the training data bucket to verify the required training files are in the bucket\nobjects = get_objects_in_bucket(cos_client,mnist_training_data_bucket)\ncontents = objects['Contents']\nfor c in contents:\n    print('file: %s ' % c['Key'])"
        }, 
        {
            "source": "### 1.4 Watson Machine Learning Client\nIn the next cell, we create a Watson Machine Learning (WML) client to save model definition, train and deploy model to WML.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create WML client to point to your WML service instance\nclient = WatsonMachineLearningAPIClient(wml_credentials)\n# Display the client version number.\nprint(client.version)"
        }, 
        {
            "source": "<a id=\"trainingdef\"></a>\n## 2. Training Definition\nDeep Learning scientist typically run many experiments to optimize various parameters of their deep learning models. \n\nAs described in [Watson Machine Learning Documentation](https://dataplatform.ibm.com/docs/content/analyze-data/ml_dlaas_working_with_experiments.html?audience=wdp&context=analytics), an experiment is a logical grouping of one or more training definitions. When an experiment is run, it creates training runs for each training definition that is part of the experiment.\n\nTraining definitions are the organizing principle for using deep learning functions in IBM Watson Machine Learning. A typical scenario might consist of dozens to hundreds of training definitions. Each training definition is defined individually and consists of the following parts: the neural network defined by using one of the supported deep learning frameworks and location of the IBM Cloud Object Storage that contains your data set. \n\nFor more details on training definitions, consult the documentation:\nhttps://dataplatform.cloud.ibm.com/docs/content/analyze-data/ml_dlaas_working_with_training_definitions.html\n\nIn the next section, we will define the training definitions to run. Note that there are some general parameters like model name, description, author, and runtime.\n\nNote the **EXECUTION_COMMAND** which effectively specifies what exactly the model consists of. In this case, the actual neural network being trained is a convolutional neural network (CNN) modeled in the **convolutional_network.py** python script. Note that in the next cell, we reference the script but didn't provide it yet. That will happen in later cells.\n\nThe model should be represented as a zip file that consists of the python script and any other dependencies. For the current example, the model consists of a zip file that includes 2 python files, one is **convolutional_network.py** and the other is **input_data.py** which handles reading in the MNIST image data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Optional if you wish to understand what are the different parameters and whether they're required or not\nclient.repository.DefinitionMetaNames.show()", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model_definition_1_metadata = {\n            client.repository.DefinitionMetaNames.NAME: \"mnist_tfmodel_hpo-nctec-v1\",\n            client.repository.DefinitionMetaNames.DESCRIPTION: \"tfmodel_description\",\n            client.repository.DefinitionMetaNames.AUTHOR_NAME: \"Joe Kozhaya\",\n            client.repository.DefinitionMetaNames.AUTHOR_EMAIL: \"kozhaya@us.ibm.com\",\n            client.repository.DefinitionMetaNames.FRAMEWORK_NAME: \"tensorflow\",\n            client.repository.DefinitionMetaNames.FRAMEWORK_VERSION: \"1.5\",\n            client.repository.DefinitionMetaNames.RUNTIME_NAME: \"python\",\n            client.repository.DefinitionMetaNames.RUNTIME_VERSION: \"3.5\",\n            client.repository.DefinitionMetaNames.EXECUTION_COMMAND: \"python3 convolutional_network.py --trainImagesFile ${DATA_DIR}/train-images-idx3-ubyte.gz --trainLabelsFile ${DATA_DIR}/train-labels-idx1-ubyte.gz --testImagesFile ${DATA_DIR}/t10k-images-idx3-ubyte.gz --testLabelsFile ${DATA_DIR}/t10k-labels-idx1-ubyte.gz --learningRate 0.001 --trainingIters 20000\"\n            }"
        }, 
        {
            "source": "Download the model files into the local runtime. There are two common locations for the model files:\n- On the web like in a github repo for example. In that case, we can download the model files using wget or urllib\n- In Cloud Object Storage. In that case, we'll download the model files using COS client.\n\nIn what follows, we download the moel files from the web, specifically the github repository https://github.com/pmservice/wml-sample-models.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Optional step in case you'd like to remove files from the present directory\nremove_all_files_from_dir('.')", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Download the model files from the web in a github repository\nfilename = 'tf-model-hpo.zip'\n# Remove file from local dir to download most recent version\nfile_path = os.path.join('.', filename)\nif os.path.exists(file_path):\n    remove_file_from_dir(filename,'.')\nfile_url = 'https://github.com/pmservice/wml-sample-models/blob/master/tensorflow/hand-written-digit-recognition/definition/tf-model-hpo.zip?raw=true'\ndownload_file_from_url(file_url,'.')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Verify the model files are downloaded to local environment\nfile_path = os.path.join('.', filename)\nif os.path.exists(file_path):\n    print(file_path)"
        }, 
        {
            "source": "#### 3.1 Model Definition in WML\nIn the next cell, we show how to store a model definition in Watson Machine Learning.\n\nSpecifically, we pass two parameters:\n1. **filename** which is a zip file that contains the required Python scripts which represent the neural network model as well as any other dependency scripts. For example, it is common to include another Python script for parsing and manipulating input data sets. \n2. **model_metadata** which defines several parameters like the deep learning framework, version,  runtime, and the execution command.\n\nIn this example notebook, we've specific the filename as **tf-model-hpo.zip** which is downloaded from a github repository and we defined the model metadata earlier, **model_definition_1_metadata**, to specify how to execute the scripts contained in the **filename**.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "definition_1_details = client.repository.store_definition(filename, model_definition_1_metadata)\n\ndefinition_1_url = client.repository.get_definition_url(definition_1_details)\ndefinition_1_uid = client.repository.get_definition_uid(definition_1_details)\nprint(definition_1_url)"
        }, 
        {
            "source": "In this case we defined and stored only one model definition. However, it is possible to define multiple model definition where different definitions could correspond to different neural network architectures or different parameters for the same NN architecture.\n\nFor example, you can define one model to apply a fully connected neural network and another model to apply a 2 layer CNN and a third model to apply a 4 layer CNN and so on. Alternatively, you can have the same NN architecture, like a 4 layer CNN, but one definition may apply one dropout value and the other definition applies a different dropout value.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Display list of stored model definitions.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "client.repository.list_definitions()"
        }, 
        {
            "source": "## Optional, if you wish to delete a specific model definition that you've stored in WML\nclient.repository.delete('a57b6f19-3e27-405d-a8d0-20ad1debff7a')", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"exp\"></a>\n## 3. Setup & Run Experiments\nPractically, data scientists would run **experiments** where each experiment consists of one or more training runs. \n\nTypically, running an experiment involves varying one or more parameters (like learning rate, convolution filter size, regularization parameter, weight initialization, ...) and recording the metric(s) of interest for the variety of these parameters.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 3.1 Experiment Parameters\nGet a list of supported config parameters for Experiments.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "client.repository.ExperimentMetaNames.show()"
        }, 
        {
            "source": "For every experiment, you need to specify where to get the input training data and where to write the training results. The input data should typically be stored in a Cloud Object Storage bucket with read permissions and the training results are typically written to a Cloud Object Storage bucket with read/write permissions.\n\nInput training data information is specified with the __TRAINING_DATA_REFERENCE__ parameter which maps to __DATA_DIR__.\n\nOutput training results is specified with the __TRAINING_RESULTS_REFERENCE__ parameter which maps to __RESULTS_DIR__.\n\nAs you can see from the Experiment config parameters command, at a minimum, the following information is required:\n- NAME which is a name for the experiment\n- TRAINING_REFERENCES\n- TRAINING_DATA_REFERENCE which specifies the location where to read the input training data. \n- TRAINING_RESULTS_REFERENCE which specifies the location where to write the training results.\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### 3.1.1 TRAINING_DATA_REFERENCE\nSpecify where to read the input training data from.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "TRAINING_DATA_REFERENCE = {\n    \"connection\": {\n                    \"endpoint_url\": service_endpoint,\n                    \"access_key_id\": cos_credentials['cos_hmac_keys']['access_key_id'],\n                    \"secret_access_key\": cos_credentials['cos_hmac_keys']['secret_access_key']\n    },\n    \"source\": {\n                \"bucket\": mnist_training_data_bucket,\n    },\n    \"type\": \"s3\"\n}"
        }, 
        {
            "source": "#### 3.1.2 TRAINING_RESULTS_REFERENCE\nSpecify where to write the training results to.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "TRAINING_RESULTS_REFERENCE = {\n    \"connection\": {\n                    \"endpoint_url\": service_endpoint,\n                    \"access_key_id\": cos_credentials['cos_hmac_keys']['access_key_id'],\n                    \"secret_access_key\": cos_credentials['cos_hmac_keys']['secret_access_key']\n    },\n    \"target\": {\n                \"bucket\": mnist_training_results_bucket,\n    },\n    \"type\": \"s3\"\n}"
        }, 
        {
            "source": "#### 3.1.3 TRAINING_REFERENCES\nNext, we need to specify the configuration parameters for the experiment.\n\nAn experiment consists of one or more training definitions and for each training definition, we can specify configuration for hyper parameter optimization.\n\nHyperparameter Optimization (HPO) is a mechanism for automatically exploring a search space of potential Hyperparameters, building a series of models and comparing the models using metrics of interest. To use HPO you must specify ranges of values to explore for each Hyperparameter.\n\nCurrently, two HPO algorithms are supported:\n- **random** implements a simple algorithm which will randomly assign Hyperparameter values from the ranges specified for an experiment.\n- **rbfopt** uses a technique called RBFOpt to explore the search space. RBFOpt is a Python library for black-box optimization (also known as derivative-free optimization). For more details, check the [user manual](https://github.com/coin-or/rbfopt/blob/master/manual.pdf)\n\nIn the configuration, we should select which algorithm to use for optimization (example below specifies **RBFOpt**), the objective to optimize (example below specifies **accuracy**), and the number of optimizer steps which sets an upper bound on the number of models which HPO will train (example below specifies **10**).\n\nNote that the **accuracy** metric needs to be a metric reported by the model. So in the **convulional_network.py**, there must be code that computes the **accuracy** metric or else HPO can't really optimize based on that metric as it needs to compare models based on that metric.\n\nThe second subsection in HPO configuration is the hyper parameters; need to specify which hyper parameters to vary and how to vary them. In the example below, we select the **learning_rate**, **dropout**, and **batch_size** as parameters to vary. \n\nNote again that these should be defined in the NN model as described in the script modeling that NN. \n\nFor more information on hyper parameter optimization:\nhttps://dataplatform.test.cloud.ibm.com/docs/content/analyze-data/ml_dlaas_hpo.html?context=wdp", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "HPO = {\n        \"method\": {\n            \"name\": \"rbfopt\", # name of the algo -- choose rbfopt\n            \"parameters\": [\n                client.experiments.HPOMethodParam(\"objective\", \"accuracy\"),\n                client.experiments.HPOMethodParam(\"maximize_or_minimize\", \"maximize\"),\n                client.experiments.HPOMethodParam(\"num_optimizer_steps\", 10)\n            ]\n        },\n        \"hyper_parameters\": [\n            client.experiments.HPOParameter('learning_rate', min=0.0001, max=0.01, step=0.0005),\n            client.experiments.HPOParameter('conv_filter_size1', min=5, max=6, step=1),\n            client.experiments.HPOParameter('conv_filter_size2', min=5, max=6, step=1),\n            client.experiments.HPOParameter('fc', min=2**9, max=2**10, step=2)\n            #client.experiments.HPOParameter('dropout', min=0.01, max=0.99, step=0.1),\n            #client.experiments.HPOParameter('batch_size', min=32, max=256, step=32)\n        ]\n     }          "
        }, 
        {
            "source": "Configure your experiment. **TRAINING_REFERENCES** links previously stored training definitions and provides information about **compute_configuration** that will be used to run the training.\n\n**Note** Change the Experiment Name so it is easier to track in the Watson Studio Experiments assets.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "experiment_metadata = {\n    client.repository.ExperimentMetaNames.NAME: \"MNIST-HPO-Experiment-NCTEC-v1\",\n    client.repository.ExperimentMetaNames.AUTHOR_NAME: \"IBM Watson\",\n    client.repository.ExperimentMetaNames.DESCRIPTION: \"MNIST Tensorflow Experiment, 1 training definition, 10 HPO models\",\n    client.repository.ExperimentMetaNames.EVALUATION_METHOD: \"multiclass\",\n    client.repository.ExperimentMetaNames.EVALUATION_METRICS: [\"accuracy\"],\n    client.repository.ExperimentMetaNames.TRAINING_DATA_REFERENCE: TRAINING_DATA_REFERENCE,\n    client.repository.ExperimentMetaNames.TRAINING_RESULTS_REFERENCE: TRAINING_RESULTS_REFERENCE,\n    client.repository.ExperimentMetaNames.TRAINING_REFERENCES: [\n        {\n            \"name\": \"HPO-MNIST\",\n            \"training_definition_url\": definition_1_url,\n            \"compute_configuration\": {\"name\": \"k80x2\"},\n            \"hyper_parameters_optimization\": HPO\n        }\n    ]\n}"
        }, 
        {
            "source": "### 3.2 Experiment Utilities\nIn the next few cells, we illustrate various experiment utilities with Watson Machine Learning. We can store an experiment in WML repository, list all stored experiments, get experiment definition, and update experiment definition.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# get the details\nexperiment_details = client.repository.store_experiment(meta_props=experiment_metadata)\n\nexperiment_uid = client.repository.get_experiment_uid(experiment_details)\nprint(experiment_uid)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# List stored experiments\n# dump the experiments w/ metadata to stdout \nclient.repository.list_experiments()"
        }, 
        {
            "source": "# Update Experiment definition if desired\nupdated_experiment_details = client.repository.update_experiment(experiment_uid, experiment_metadata)", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import json\n# Get Experiment definition\n#details is a python dict\ndetails = client.repository.get_experiment_details(experiment_uid)\nprint(json.dumps(details, indent=2))"
        }, 
        {
            "source": "Delete experiment definition from repository\n# this cell is not active \nclient.repository.delete(experiment_uid)", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "source": "### 3.3 Running Experiments\nOnce an experiment is defined, we can run the experiment and monitor the details", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Run the experiment\nexperiment_run_details = client.experiments.run(experiment_uid, asynchronous=True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# List experiment runs\nclient.experiments.list_runs()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Get experiment run uid\nexperiment_run_uid = client.experiments.get_run_uid(experiment_run_details)\nprint(experiment_run_uid)"
        }, 
        {
            "source": "**LIST training runs triggered by experiment run**\n\nRun the cell below several times during the run to see updates or monitor for a few minutes.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# query the service ~ every minute\n### OPTIONAL\n####import time \n\n####f = lambda x: time.sleep(6) if x%10!=0 else client.experiments.list_training_runs(experiment_run_uid)\n####[f(i) for i in range (60)]", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "source": "# Get experiment run details\nexperiment_run_details = client.experiments.get_run_details(experiment_run_uid)\nprint(json.dumps(experiment_run_details, indent=2))", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Get experiment run status\n# It is useful to understand the status of the experiment while it runs in the background\nclient.experiments.get_status(experiment_run_uid)"
        }, 
        {
            "source": "# Print Experiment details\nexperiment_details = client.experiments.get_details(experiment_uid)\nprint(json.dumps(experiment_details, indent=2))", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Get Training Run UID\nexperiment_run_details = client.experiments.get_run_details(experiment_run_uid)\ntraining_run_uids = client.experiments.get_training_uids(experiment_run_details)\n\nfor i in training_run_uids:\n    print(i)"
        }, 
        {
            "source": "### 3.4 Monitoring Experiments\nOnce an experiment run is triggered, use the following utilities to monitor a running experiment.\n\nYou can monitor experiment run by calling client.experiments.monitor_logs(run_uid). This method will stream training logs content to console.\n\nTip: You can also monitor particular training run by calling client.training.monitor_logs(training_run_uid). To get training_run_uid you can call method client.experiments.list_training_runs(experiment_run_uid)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Optional\nclient.experiments.monitor_logs(experiment_run_uid)", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "source": "# Optional\ntraining_run_uid = 'SPECIFY THE TRAINING RUN UID '\nclient.training.monitor_logs(training_run_uid)", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "source": "### 3.5 Evaluation metrics\n\nYou can get final evaluation metrics by running the cell below.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "metrics = client.experiments.get_latest_metrics(experiment_run_uid)\nprint(json.dumps(metrics, indent=2))", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "source": "all_metrics = client.experiments.get_metrics(experiment_run_uid)\nprint(json.dumps(all_metrics, indent=2))", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "source": "### 3.6 Visualization of Experiment Results\nIt is important to visualize the results of your experiments so you can understand the performance of your NN architecture as well as decide on next steps as you explore different architectures and/or parameters for your problem.\n\nOne popular approach is to visualize the results of your experiments by using the Experiment Assistant in Watson Studio. For this to work, you need to add your Watson Machine Learning to your Watson Studio project:\n- Navigate to your project's home\n- On your project page, click on the **Settings** tab.\n- On the **Settings** tab, scroll down to **Associated services** and click on  __Add service__ drop down (on the right side of the page) and select **Watson**.\n- This pops a window with all the Watson services including the Machine Learning service. Click the Add link in the Machine Learning service tile.\n- On the page that loads, select the __Existing__ tab and select the specific Machine Learning service where you ran your experiment.\n\nOnce you've associated the correct Watson Machine Learning service with your project, you can visualize the experiment results using the Experiment Assistant:\n- Navigate to your project's home\n- On your project page, click on the **Assets** tab.\n- Scroll down to **Experiments** and click on the name of the experiment you just ran.\n- This loads the Experiment Assistant which shows an  __Overview__ of the experiment, the associated __Training Runs__, and the experiment metrics with charts under __Compare Runs__ tab showing how the metrics change for the different runs.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"store\"></a>\n## 4. Store & Deploy Trained ML Models\nNow that we have train a model using DLaaS and selected the run with best accuracy metric, next step is to store the trained model and deploy it to REST API that developers can incorporate in their applications.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "status = client.experiments.get_status(experiment_run_uid)\nbest_model_uid = status['best_results']['experiment_best_model']['training_guid']\nbest_model_name = status['best_results']['experiment_best_model']['training_reference_name']\n\nprint(best_model_uid + ' (' +  best_model_name  + ')')"
        }, 
        {
            "source": "### 4.1 Save Model in WML", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "saved_model_details = client.repository.store_model(best_model_uid, {'name': 'NCTEC v3 best model MNIST'})"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model_uid            = client.repository.get_model_uid( saved_model_details )\nprint( \"model_uid: \", model_uid )"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "client.repository.list_models()"
        }, 
        {
            "source": "### 4.2 Deploy ML Model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "deployment_details = client.deployments.create(model_uid, name=\"MNIST Tensorflow deployment NCTEC v3\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "scoring_url = client.deployments.get_scoring_url(deployment_details)\nprint(scoring_url)"
        }, 
        {
            "source": "### 4.3 Score Data using Deployed ML Model\nIn the next section, we'll illustrate how to use the trained and deployed model to predict some test images.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Load MNIST images into train and test variables\nfrom keras.datasets import mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Plot two images to validate the digitis they represent\nfor i, image in enumerate([x_test[0], x_test[1]]):\n    plt.subplot(2, 2, i + 1)\n    plt.axis('off')\n    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#print(x_test[0].shape)\nprint(x_test[0])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "image1 = x_test[0]\nimage2 = x_test[1]\nimg1 = image1.reshape(image1.shape[0]*image1.shape[1])\nimg2 = image2.reshape(image2.shape[0]*image2.shape[1])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(img1.shape)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "scoring_data = {'values': [img1.tolist(), img2.tolist()]}"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Predict which digit the image represents using the trained and deployed machine learning model\npredictions = client.deployments.score(scoring_url, scoring_data)\nprint(\"Scoring result: \" + str(predictions))"
        }, 
        {
            "source": "<a id=\"summary\"></a>\n## 5. Summary\nThis notebook outlined how to run deep learning experiments from Jupyter Notebooks in Watson Studio.\nCheck out the [Documentation](https://dataplatform.cloud.ibm.com/docs/content/analyze-data/wml-setup.html) for further details.\n\nAlso, the following [github repository](https://github.com/biosopher/unofficial-watson-studio-python-utils) has great assets and utilities to simplify setting up and running deep learning experiments using Watson Machine Learning.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Authors\n**Joe Kozhaya** is an IBM Master Inventor and WorldWide Enablement lead for Watson Data & AI solutions.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Copyright \u00a9 2017, 2018 IBM. This notebook and its source code are released under the terms of the MIT License.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}